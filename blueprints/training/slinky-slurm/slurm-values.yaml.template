# SPDX-FileCopyrightText: Copyright (C) SchedMD LLC.
# SPDX-License-Identifier: Apache-2.0

# -------------------------
# Common affinity for non-compute components
# -------------------------
commonAffinity: &commonAffinity
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
          - key: "node.kubernetes.io/instance-type"
            operator: In
            values:
              - "m5.4xlarge"
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: "app.kubernetes.io/name"
            operator: In
            values: ["slurmdbd", "slurmctld", "slurm-exporter", "login", "mariadb", "slurmrestd"]
        topologyKey: "kubernetes.io/hostname"

# Optimized Slurm config files supporting Pyxis + Enroot
configFiles:
  cgroup.conf: |
    # Use autodetect to automatically select cgroup plugin
    CgroupPlugin=autodetect
    # Do not enforce systemd constraints (more portable for container workloads)
    IgnoreSystemd=yes
    # Resource constraints
    ConstrainCores=yes
    ConstrainRAMSpace=yes
    ConstrainSwapSpace=yes
    ConstrainDevices=yes
    ConstrainKill=yes               # Kill tasks on step end
    AllowedSwapSpace=0              # Limit swap usage
    TaskAffinity=yes                # Enable CPU pinning for container tasks

  gres.conf: |
    # Automatic detection of NVIDIA GPUs
    Name=gpu Type=any File=/dev/nvidiactl
    NodeName=DEFAULT Name=gpu Count=AutoDetect

  job_container.conf: |
    # Enroot + Pyxis configuration
    AutoBasePath=true               # Automatically mount base paths
    EntireStepInNS=true             # Run entire job step in namespaces
    Plugin=pyxis                     # Use Pyxis to manage container runtime
    AllowUtmp=no                     # Disable writing to utmp/wtmp
    EnrootCache=/var/tmp/enroot     # Recommended Enroot cache path
    EnrootTemporary=/tmp             # Temporary writable path

  mpi.conf: |
    # Minimal MPI debug info for production
    PMIxDebug=0
    # Enable proper resource reporting
    ResvPortRange=50000-51000

  oci.conf: |
    # Enable debug for OCI-based container runtime
    FileDebug=debug2
    # Optionally tune OCI runtimes
    Runtime=Enroot

  plugstack.conf: |
    # Pyxis plugstack includes
    include /usr/share/pyxis/*


# -------------------------
# Global settings
# -------------------------
imagePullPolicy: IfNotPresent
imagePullSecrets: []

# -------------------------
# Slurm Controller (slurmctld)
# -------------------------
controller:
  persistence:
    storageClassName: gp3
  podSpec:
    affinity: *commonAffinity

# -------------------------
# Slurm Accounting (slurmdbd)
# -------------------------
accounting:
  enabled: true
  podSpec:
    affinity: *commonAffinity

# -------------------------
# Slurm REST API
# -------------------------
restapi:
  podSpec:
    affinity: *commonAffinity

# -------------------------
# Slurm Exporter
# -------------------------
slurm-exporter:
  exporter:
    affinity: *commonAffinity

# -------------------------
# Login nodes
# -------------------------
loginsets:
  slinky:
    enabled: true
    replicas: 1
    login:
      volumeMounts:
        - name: fsx-lustre
          mountPath: /fsx
    rootSshAuthorizedKeys:
      - ${ssh_key}
    extraSshdConfig:
      AuthorizedKeysFile: "/root/.ssh/authorized_keys"
      ChallengeResponseAuthentication: "no"
      PasswordAuthentication: "no"
      PubkeyAuthentication: "yes"
      PermitRootLogin: "yes"
      Port: "22"
    podSpec:
      affinity: *commonAffinity
      volumes:
        - name: fsx-lustre
          persistentVolumeClaim:
            claimName: fsx-static-pvc


# -------------------------
# Slurm Compute nodes
# -------------------------
nodesets:
  slinky:
    enabled: true
    replicas: 2
    slurmd:
      image:
        repository: ${image_repository}
        tag: ${image_tag}
      resources:
        limits:
          nvidia.com/gpu: "1"
          vpc.amazonaws.com/efa: 1
        requests:
          nvidia.com/gpu: "1"
          vpc.amazonaws.com/efa: 1
      nodeSelector:
        instanceType: g5-gpu-karpenter
        kubernetes.io/os: linux
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      volumeMounts:
        - name: fsx-lustre
          mountPath: /fsx
        - name: shmem
          mountPath: /dev/shm
    podSpec:
      affinity: {}
      nodeSelector:
        kubernetes.io/os: linux
      volumes:
        - name: fsx-lustre
          persistentVolumeClaim:
            claimName: fsx-static-pvc
        - name: shmem
          hostPath:
            path: /dev/shm
    partition:
      enabled: true


# -------------------------
# Vendor-specific configurations
# -------------------------
vendor:
  nvidia:
    dcgm:
      enabled: true



